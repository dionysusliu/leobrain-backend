# scripts/trigger_crawlers.py
#!/usr/bin/env python3
"""
æ‰¹é‡è§¦å‘çˆ¬è™«ä»»åŠ¡è„šæœ¬ï¼ˆç›´æ¥ä½¿ç”¨ Prefectï¼‰

ä½¿ç”¨æ–¹æ³•:
    # è§¦å‘æ‰€æœ‰ç«™ç‚¹
    python scripts/trigger_crawlers.py

    # è§¦å‘æŒ‡å®šç«™ç‚¹
    python scripts/trigger_crawlers.py --sites bbc hackernews techcrunch

    # å¹¶è¡Œè§¦å‘ï¼ˆé»˜è®¤ä¸²è¡Œï¼‰
    python scripts/trigger_crawlers.py --parallel
"""
import os
import sys
import asyncio
import argparse
import logging
from pathlib import Path
from typing import List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from flows.crawler_deployments import trigger_manual_crawl
from configs import load_site_configs

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def trigger_single_crawl(site_name: str) -> dict:
    """è§¦å‘å•ä¸ªç«™ç‚¹çš„çˆ¬è™«ä»»åŠ¡"""
    try:
        flow_run_id = await trigger_manual_crawl(site_name)
        return {
            "success": True,
            "site": site_name,
            "flow_run_id": flow_run_id
        }
    except Exception as e:
        logger.error(f"Error triggering {site_name}: {e}")
        return {
            "success": False,
            "site": site_name,
            "error": str(e)
        }


async def batch_trigger_crawl(
    sites: Optional[List[str]] = None,
    parallel: bool = False
) -> dict:
    """æ‰¹é‡è§¦å‘çˆ¬è™«ä»»åŠ¡"""
    # è·å–æ‰€æœ‰ç«™ç‚¹é…ç½®
    all_sites = load_site_configs()
    
    # ç¡®å®šè¦è§¦å‘çš„ç«™ç‚¹
    if sites is None:
        sites = list(all_sites.keys())
    
    # éªŒè¯ç«™ç‚¹æ˜¯å¦å­˜åœ¨
    invalid_sites = [s for s in sites if s not in all_sites]
    if invalid_sites:
        raise ValueError(f"Invalid sites: {', '.join(invalid_sites)}")
    
    results = {}
    
    if parallel:
        # å¹¶è¡Œè§¦å‘
        tasks = [trigger_single_crawl(site) for site in sites]
        results_list = await asyncio.gather(*tasks)
        for result in results_list:
            results[result["site"]] = result
    else:
        # ä¸²è¡Œè§¦å‘
        for site in sites:
            result = await trigger_single_crawl(site)
            results[result["site"]] = result
    
    total = len(sites)
    success = sum(1 for r in results.values() if r["success"])
    failed = total - success
    
    return {
        "total": total,
        "success": success,
        "failed": failed,
        "results": results
    }


async def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è§¦å‘çˆ¬è™«ä»»åŠ¡")
    parser.add_argument(
        "--sites",
        nargs="+",
        help="è¦è§¦å‘çš„ç«™ç‚¹åç§°åˆ—è¡¨ï¼ˆä¾‹å¦‚: bbc hackernews techcrunchï¼‰"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="è§¦å‘æ‰€æœ‰é…ç½®çš„ç«™ç‚¹"
    )
    parser.add_argument(
        "--parallel",
        action="store_true",
        help="å¹¶è¡Œè§¦å‘æ‰€æœ‰ä»»åŠ¡ï¼ˆé»˜è®¤ä¸²è¡Œï¼‰"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½® Prefect API URL
    api_url = os.getenv("PREFECT_API_URL", "http://localhost:4200/api")
    os.environ["PREFECT_API_URL"] = api_url
    
    # ç¡®å®šè¦è§¦å‘çš„ç«™ç‚¹
    if args.all:
        sites = None  # None è¡¨ç¤ºè§¦å‘æ‰€æœ‰ç«™ç‚¹
        print("ğŸ“‹ å°†è§¦å‘æ‰€æœ‰é…ç½®çš„ç«™ç‚¹")
    elif args.sites:
        sites = args.sites
        print(f"ğŸ“‹ å°†è§¦å‘ä»¥ä¸‹ç«™ç‚¹: {', '.join(sites)}")
    else:
        # é»˜è®¤è§¦å‘æ‰€æœ‰ç«™ç‚¹
        sites = None
        print("ğŸ“‹ æœªæŒ‡å®šç«™ç‚¹ï¼Œå°†è§¦å‘æ‰€æœ‰é…ç½®çš„ç«™ç‚¹")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ --sites æŒ‡å®šç«™ç‚¹ï¼Œæˆ–ä½¿ç”¨ --all æ˜ç¡®è§¦å‘æ‰€æœ‰ç«™ç‚¹")
    
    # æ˜¾ç¤ºæ‰§è¡Œæ¨¡å¼
    mode = "å¹¶è¡Œ" if args.parallel else "ä¸²è¡Œ"
    print(f"âš™ï¸  æ‰§è¡Œæ¨¡å¼: {mode}")
    print(f"ğŸŒ Prefect API: {api_url}\n")
    
    try:
        # è§¦å‘ä»»åŠ¡
        result = await batch_trigger_crawl(
            sites=sites,
            parallel=args.parallel
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š æ‰§è¡Œç»“æœ")
        print("="*60)
        print(f"æ€»è®¡: {result['total']} ä¸ªä»»åŠ¡")
        print(f"âœ… æˆåŠŸ: {result['success']} ä¸ª")
        print(f"âŒ å¤±è´¥: {result['failed']} ä¸ª")
        print("\nè¯¦ç»†ç»“æœ:")
        
        for site_name, site_result in result['results'].items():
            if site_result.get('success'):
                flow_run_id = site_result.get('flow_run_id', 'N/A')
                print(f"  âœ… {site_name}: æˆåŠŸ (Flow Run ID: {flow_run_id})")
            else:
                error = site_result.get('error', 'Unknown error')
                print(f"  âŒ {site_name}: å¤±è´¥ - {error}")
        
        print("\n" + "="*60)
        print("ğŸ’¡ æç¤º: åœ¨ Prefect WebUI (http://localhost:4200) æŸ¥çœ‹ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€")
        print("="*60)
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        sys.exit(0 if result['failed'] == 0 else 1)
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())